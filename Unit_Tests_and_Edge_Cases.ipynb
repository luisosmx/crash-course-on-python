{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhHVNfRsZhEBLiF+FlNa1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisosmx/crash-course-on-python/blob/main/Unit_Tests_and_Edge_Cases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we have some code that makes a list of specific letters found in any string. If you run it, you can see what it does."
      ],
      "metadata": {
        "id": "JkD4nnBKM2qX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IMEX5MqM0Wb",
        "outputId": "39cbdee2-4c8c-431e-fe5a-5d9263866172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b']\n"
          ]
        }
      ],
      "source": [
        "import re \n",
        "  \n",
        "my_txt = \"An investment in knowledge pays the best interest.\"\n",
        "\n",
        "def LetterCompiler(txt):\n",
        "    result = re.findall(r'([a-c]).', txt)\n",
        "    return result\n",
        "\n",
        "print(LetterCompiler(my_txt))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output, you can see that the LetterCompiler( ) function finds all matches for the letters a through c in an input string if followed by another character and returns them as a list of strings, with each string representing one match. Nice.\n",
        "\n",
        "But can we be sure that this function will always do what we expect it to do? We need to write code to help us catch mistakes, errors and bugs. This code should automate the process of checking if the returned value of our code matches the expectations by dynamically feeding into it test cases. Since we're dynamically feeding in different strings, it would be prudent to create unit tests for our code. We can use the module unittest for this.\n",
        "\n",
        "Fill in the blanks below to create an automatic unit test that verifies whether input strings have the correct list of string matches."
      ],
      "metadata": {
        "id": "qxmhufJQM8YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestCompiler(unittest.TestCase):\n",
        "\n",
        "    def test_basic(self):\n",
        "        testcase = \"The best preparation for tomorrow is doing your best today.\"\n",
        "        expected = ['b', 'a', 'a', 'b', 'a']\n",
        "        self.assertEqual(LetterCompiler(testcase), expected)"
      ],
      "metadata": {
        "id": "j9cg-M-VM7y8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your automatic test is coded, you need to call the unittest.main( ) function to run the test. It is important to note that the configuration for running unit tests in Jupyter is different than running unit tests from the command line. Running unittest.main( ) in Jupyter will result in an error. You can see this by runnig the following cell to execute your automatic test."
      ],
      "metadata": {
        "id": "ED7WkZr4NDEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "DMoyJWPMNCCx",
        "outputId": "5d617d91-9a98-468a-91a2-ab0788bd7816"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E\n",
            "======================================================================\n",
            "ERROR: /root/ (unittest.loader._FailedTest)\n",
            "----------------------------------------------------------------------\n",
            "AttributeError: module '__main__' has no attribute '/root/'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.002s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yikes! SystemExit: True means an error occurred, as expected. The reason is that unittest.main( ) looks at sys.argv. In Jupyter, by default, the first parameter of sys.argv is what started the Jupyter kernel which is not the case when executing it from the command line. This default parameter is passed into unittest.main( ) as an attribute when you don't explicitly pass it attributes and is therefore what causes the error about the kernel connection file not being a valid attribute. Passing an explicit list to unittest.main( ) prevents it from looking at sys.argv.\n",
        "\n",
        "Let's pass it the list ['first-arg-is-ignored'] for example. In addition, we will pass it the parameter exit = False to prevent unittest.main( ) from shutting down the kernel process. Run the following cell with the argv and exit parameters passed into unittest.main( ) to rerun your automatic test."
      ],
      "metadata": {
        "id": "8q_NiLAiNKwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv = ['first-arg-is-ignored'], exit = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahHIMPzaNGm7",
        "outputId": "55720631-92aa-4db9-90ec-cb11e30a01d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.002s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f1a89b05fc0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edge cases are inputs to code that produce unexpected results, and are found at the extreme ends of the ranges of input we imagine programs will typically work with. Can you use the cell below to write some edge cases? We've already filled in another test case for you! As it is, this test will run fine. Can you come up with at least one test case that you think could result in a wrong return value? No wrong answers! Feel free to play around.\n"
      ],
      "metadata": {
        "id": "u2NMRMcNNTcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestCompiler2(unittest.TestCase):\n",
        "    \n",
        "    def test_two(self):\n",
        "        testcase = \"A b c d e f g h i j k l m n o q r s t u v w x y z\"\n",
        "        expected = ['b', 'c']\n",
        "        self.assertEqual(LetterCompiler(testcase), expected)\n",
        "\n",
        "# EDGE CASES HERE\n",
        "\n",
        "unittest.main(argv = ['first-arg-is-ignored'], exit = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "incHeTFMNUu6",
        "outputId": "fad256be-4273-4cb3-9650-88ebaff5140e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.004s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f1a89b06110>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you find any edge cases? If not, continue working on it. Choosing test cases can be an exercise in creativity. Coming up with different ways a code might break can be super fun! When you have found an edge case, think about special handling in your script in order for your code to continue to behave correctly.\n",
        "\n",
        "If you are out of ideas: Try removing the spaces and figure out why they were in the example testcase. Does that give you an idea for other tests?\n",
        "\n",
        "When you have found at least one edge case, you are all done with this notebook. You should take a moment to reflect on what you've done so far. It's super impressive and it's going to fit nicely in your IT toolkit."
      ],
      "metadata": {
        "id": "jjLOTw0SNYym"
      }
    }
  ]
}